{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f0969c",
   "metadata": {},
   "source": [
    "# 套件載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f3f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38881684",
   "metadata": {},
   "source": [
    "# 讀取資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddef7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('nlp-getting-started/train.csv')\n",
    "test_data = pd.read_csv('nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0dc44b",
   "metadata": {},
   "source": [
    "# 預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bb6ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_preprocess(text):\n",
    "    \"\"\"移除 '%20'\"\"\"\n",
    "    if pd.notnull(text):\n",
    "        text = text.replace(\"%20\", \" \")\n",
    "    else:\n",
    "        text = ''\n",
    "    return text\n",
    "\n",
    "def remove_url(text):\n",
    "    url_pattern = re.compile(r'https?://t\\.co/[^\\s]*')\n",
    "    new_text = url_pattern.sub('', text)\n",
    "    return new_text\n",
    "\n",
    "def remove_at(text):\n",
    "    at_pattern = re.compile(r'@[^\\s]*')\n",
    "    new_text = at_pattern.sub('', text)\n",
    "    return new_text\n",
    "\n",
    "def text_preprocess(text):\n",
    "    \"\"\"移除 url、@xxx\"\"\"\n",
    "    text = remove_url(text)\n",
    "    text = remove_at(text)\n",
    "    return text\n",
    "\n",
    "# remove url and @ from text\n",
    "data['text'] = data['text'].apply(text_preprocess)\n",
    "test_data['text'] = test_data['text'].apply(text_preprocess)\n",
    "\n",
    "# remove %20 from keyword\n",
    "data['keyword'] = data['keyword'].apply(keyword_preprocess)\n",
    "test_data['keyword'] = test_data['keyword'].apply(keyword_preprocess)\n",
    "\n",
    "# combine keyword and text\n",
    "data['keyword_text'] = data.apply(lambda row: row['keyword'] + ' ' + row['text'], axis=1)\n",
    "test_data['keyword_text'] = test_data.apply(lambda row: row['keyword'] + ' ' + row['text'], axis=1)\n",
    "\n",
    "train_data_dict = {\n",
    "    \"text\": data[\"keyword_text\"].tolist(),\n",
    "    \"label\": data[\"target\"].tolist()\n",
    "}\n",
    "\n",
    "test_data_dict = {\n",
    "    \"text\": test_data[\"keyword_text\"].tolist()\n",
    "}\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data_dict)\n",
    "test_dataset = Dataset.from_dict(test_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10d4311",
   "metadata": {},
   "source": [
    "# 載入預訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd390773",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# use dynamic padding \n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bb74ec",
   "metadata": {},
   "source": [
    "# 訓練並輸出結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa5f331f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kerueilin9\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1904' max='1904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1904/1904 01:16, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.500300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.435300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.382900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"test-trainer\",\n",
    "    report_to='none',\n",
    "    num_train_epochs=2,\n",
    "    save_strategy = \"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(tokenized_test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "submission = pd.DataFrame({'id':test_data['id'],'target':preds})\n",
    "submission.to_csv('nlp-getting-started/submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a23ea0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  target\n",
      "0                    Just happened a terrible car crash       1\n",
      "1     Heard about #earthquake is different cities, s...       1\n",
      "2     there is a forest fire at spot pond, geese are...       1\n",
      "3              Apocalypse lighting. #Spokane #wildfires       1\n",
      "4         Typhoon Soudelor kills 28 in China and Taiwan       1\n",
      "...                                                 ...     ...\n",
      "3258  EARTHQUAKE SAFETY LOS ANGELES ??? SAFETY FASTE...       0\n",
      "3259  Storm in RI worse than last hurricane. My city...       1\n",
      "3260                  Green Line derailment in Chicago        1\n",
      "3261        MEG issues Hazardous Weather Outlook (HWO)        1\n",
      "3262  #CityofCalgary has activated its Municipal Eme...       1\n",
      "\n",
      "[3263 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "test_data_location = pd.read_csv('nlp-getting-started/test.csv')\n",
    "test_data_location['text'] = test_data_location['text'].apply(text_preprocess)\n",
    "test_data_location['keyword'] = test_data_location['keyword'].apply(keyword_preprocess)\n",
    "test_data_location['keyword_text'] = test_data_location.apply(lambda row: row['keyword'] + ' ' + row['text'], axis=1)\n",
    "test_dataset_location = test_data_location[['keyword_text']]\n",
    "\n",
    "test_data_dict_location = {\n",
    "    \"text\": test_dataset_location[\"keyword_text\"].tolist()\n",
    "}\n",
    "test_dataset_location = Dataset.from_dict(test_data_dict_location)\n",
    "tokenized_test_dataset_location = test_dataset_location.map(tokenize_function, batched=True)\n",
    "predictions = trainer.predict(tokenized_test_dataset_location)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "result = pd.DataFrame({'text':test_data_location['text'],'target':preds})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "faaf81c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  target  \\\n",
      "4         Typhoon Soudelor kills 28 in China and Taiwan       1   \n",
      "15    Birmingham Wholesale Market is ablaze BBC News...       1   \n",
      "34      Accident on A27 near Lewes is it Kingston Ro...       1   \n",
      "36    For Legal and Medical Referral Service  Call u...       1   \n",
      "52    'We are still living in the aftershock of Hiro...       1   \n",
      "...                                                 ...     ...   \n",
      "3238  Wreckage 'Conclusively Confirmed' as From MH37...       1   \n",
      "3239  Wreckage 'Conclusively Confirmed' as From MH37...       1   \n",
      "3254  Officials: Alabama home quarantined over possi...       1   \n",
      "3257  The death toll in a #IS-suicide car bombing on...       1   \n",
      "3260                  Green Line derailment in Chicago        1   \n",
      "\n",
      "                   location  \n",
      "4                     China  \n",
      "15               Birmingham  \n",
      "34                    Lewes  \n",
      "36                    Legal  \n",
      "52                Hiroshima  \n",
      "...                     ...  \n",
      "3238               Wreckage  \n",
      "3239               Wreckage  \n",
      "3254                Alabama  \n",
      "3257  the Village of Rajman  \n",
      "3260                Chicago  \n",
      "\n",
      "[441 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "def location_detect(text):\n",
    "    doc = nlp(text)\n",
    "    data = [(X.text, X.label_) for X in doc.ents]\n",
    "    for word, pos in data:\n",
    "        if pos == 'GPE':\n",
    "            return(word)\n",
    "test_data_location['location'] = test_data_location['text'].apply(location_detect)\n",
    "# print(test_data_location['location'])\n",
    "result = pd.DataFrame({'text':test_data_location['text'],'target':preds,'location':test_data_location['location']})\n",
    "result = result[result['target'] == 1]\n",
    "result = result[result['location'].notnull()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6404429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92120275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
